{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr>\n",
    "    <td width=10%>\n",
    "        <img src=\"images/eisti_logo.png\">\n",
    "    </td>\n",
    "    <td>\n",
    "        <center>\n",
    "            <h1>Deep Learning et Applications</h1>\n",
    "        </center>\n",
    "    </td>\n",
    "    <td width=15%>\n",
    "        Yann Vernaz  et Paul Gay\n",
    "    </td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "<br/>\n",
    "<div id=\"top\"></div>\n",
    "<center>\n",
    "    <a style=\"font-size: 20pt; font-weight: bold\">Structure from motion with objects</a>\n",
    "</center></a>\n",
    "<br/>\n",
    "\n",
    "Ce TP vous propose d'implémenter une chaine de traitement complète de structure from motion. Nous allons voir comment des méthodes de Deep Learning peuvent nous aider à obtenir une représentation \"sémantique\" de la scène en détectant les objets qui la composent. Nous appliquerons ensuite les principes de géométrie vue en classe pour obtenir une représentation 3D de notre séquence d'images. Ceci nous permettra d'avoir un premier exemple de vision par ordinateur.\n",
    "\n",
    "<br/>\n",
    "\n",
    "\n",
    "<br/>\n",
    "&nbsp;&nbsp;&nbsp; 1) <a href=\"#2D3D\"> De la 3D vers la 2D : Comprendre les matrices de projections </a><br/>\n",
    "\n",
    "Le but de cette partie est de se familiariser avec les formes géomètriques des coniques et des quadriques qui seront utilisées pour représenter des objets. L'intérêt de cette représentation est que les relations de projection entre la 3D et la 2D peuvent être modélisée relativement facilement par des matrices de caméra orthographiques.\n",
    "\n",
    "&nbsp;&nbsp;&nbsp; 2) <a href=\"#yolo\"> Mesures : détecter les objets sur les images </a><br/>\n",
    "Il s'agit ensuite de détecter les objets dans des images. Nous allons pour cela utiliser un réseau de neurones convolutionnel célèbre pour sa rapidité : YOLO. \n",
    "\n",
    "&nbsp;&nbsp;&nbsp; 3) <a href=\"#tracking\"> Tracking : Associer les détections entre les images </a><br/>\n",
    "Comme pour les correspondances entre les points, nous allons associer les apparitions du même objet entre différentes images.\n",
    "\n",
    "&nbsp;&nbsp;&nbsp; 4) <a href=\"#sfmo\"> Retrouver les ellipsoides 3D</a><br/>\n",
    "A présent que nous avons obtenus nos observations, il s'agit de mettre en forme le système d'équations et de le résoudre. Les étapes sont décrites dans le cours et sur ce notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <a id=\"3D2D\"> 1) Comprendre les matrices de projections </a>\n",
    "\n",
    "Prenez le temps de vous familiariser avec les fonctions en générant et visualisant des données synthétiques.\n",
    "\n",
    "Note que vous pouvez avoir un meilleur rendu en exécutant le script depuis un terminal. En effet, les fontions de visualisation 3D sont mal gếrées par le notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tp_geometry import gen_synth\n",
    "from tp_geometry import visualize\n",
    "import numpy as np\n",
    "\n",
    "# Génére des données synthétiques avec 5 frames et 3 ellipsoides et une caméra orthographique\n",
    "n_o = 3 # number of objects\n",
    "n_f = 5 # number of frames\n",
    "(Ps, Qs, Cs) = gen_synth.get_conics_quadrics_cameras(typ='orth', n_f=n_f, n_o = n_o)\n",
    "colors = np.random.rand(n_o, 3) # setting the colors of the ellipsoids\n",
    "visualize.plot_ellipsoids(Qs, colors = colors, keep_alive=True)\n",
    "visualize.plot_ellipses(Cs, colors = colors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vérfier la relation entre les caméras, les coniques et les quadriques, c'est à dire l'équation : \n",
    "$$C = PQP^T $$\n",
    "\n",
    "Les matrices sont stockées par blocs dans les variables `Ps`, `Cs` et `Qs`.\n",
    "```\n",
    "Ps[:2,:] # deux premières lignes de la matrice de rotation de la caméra pour la première image\n",
    "Cs[:3,:3] # conique du premier objet dans la première image\n",
    "Qs[:4,:] # quadrique du premier objet\n",
    "```\n",
    "\n",
    "Note: afin d'effectuer l'opération, vous devez récupérer les deux lignes et ajouter des 1 et 0 comme indiquer dans le cours pour que les dimensions correspondent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Le premier conique de la première image\")\n",
    "print(C[:3,:3])\n",
    "print(\"la reprojection du quadrique\")\n",
    "# votre code ici\n",
    "reprojection = \n",
    "print(reprojection/(-reprojection[2,2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <a id=\"yolo\"> 2) Mesures : détecter les objets sur les images</a>\n",
    "\n",
    "À l'heure actuelle le modèle Yolo est l'un des réseaux de neurones les plus utilisés pour détecter des objets dans les images. En particulier, c'est l'un des plus rapides. Il sera détaillé dans le cours sur la détection d'objets. \n",
    "\n",
    "Téléchargez les poids du réseau de neurones yolov3. Ils ont déjà été optimisés sur des bases standards de vision par ordinateur et peuvent être utilisés directement pour l'inférence :\n",
    "```\n",
    "wget https://pjreddie.com/media/files/yolov3.weights\n",
    "```\n",
    "\n",
    "Nous allons à présent le tester rapidement sur une image. Tout d'abord nous devons écrire les fonctions qui appliqueront l'inférence, et afficheront les résultats sur l'image sous forme de boites englobantes. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt, matplotlib\n",
    "\n",
    "# function to get the output layer names \n",
    "# in the architecture\n",
    "def get_output_layers(net):\n",
    "    layer_names = net.getLayerNames()\n",
    "    output_layers = [layer_names[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
    "    return output_layers\n",
    "\n",
    "def get_detection(image, net):\n",
    "    Width = image.shape[1]\n",
    "    Height = image.shape[0]\n",
    "    scale = 0.00392\n",
    "   # create input blob \n",
    "    blob = cv2.dnn.blobFromImage(image, scale, (416,416), (0,0,0), True, crop=False)\n",
    "    # set input blob for the network\n",
    "    net.setInput(blob)\n",
    "    # run inference through the network\n",
    "    # and gather predictions from output layers\n",
    "    outs = net.forward(get_output_layers(net))\n",
    "    # initialization\n",
    "    class_ids = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "    conf_threshold = 0.5\n",
    "    nms_threshold = 0.4\n",
    "\n",
    "    # for each detetion from each output layer \n",
    "    # get the confidence, class id, bounding box params\n",
    "    # and ignore weak detections (confidence < 0.5)\n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            if confidence > 0.5:\n",
    "                center_x = int(detection[0] * Width)\n",
    "                center_y = int(detection[1] * Height)\n",
    "                w = int(detection[2] * Width)\n",
    "                h = int(detection[3] * Height)\n",
    "                x = center_x - w / 2\n",
    "                y = center_y - h / 2\n",
    "                class_ids.append(class_id)\n",
    "                confidences.append(float(confidence))\n",
    "                boxes.append([x, y, w, h])\n",
    "    # apply non maxima suppression to remove duplicate detections \n",
    "    # i.e. the same object has been detected multiple times\n",
    "    indices = cv2.dnn.NMSBoxes(boxes, confidences, conf_threshold, nms_threshold)\n",
    "    boxes = [boxes[i[0]] for i in indices]\n",
    "    confidences = [confidences[i[0]] for i in indices]\n",
    "    class_ids = [class_ids[i[0]] for i in indices]\n",
    "    return boxes, confidences, class_ids\n",
    "\n",
    "\n",
    "# function to draw bounding box on the detected object with class name\n",
    "def draw_bounding_box(img, label, x, y, x_plus_w, y_plus_h, color, width=5):\n",
    "    cv2.rectangle(img, (x,y), (x_plus_w,y_plus_h), color, width)\n",
    "    cv2.putText(img, label, (x-10,y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "    \n",
    "def draw_detections(image, class_ids, classes, boxes, COLORS):\n",
    "    # generate different colors for different classes \n",
    "    # go through the detections remaining\n",
    "    # after nms and draw bounding box\n",
    "    for (i,box) in enumerate(boxes):\n",
    "        box = boxes[i]\n",
    "        x = box[0]\n",
    "        y = box[1]\n",
    "        w = box[2]\n",
    "        h = box[3]\n",
    "        class_id = class_ids[i]\n",
    "        label = classes[class_id]\n",
    "        color = COLORS[class_id]\n",
    "        draw_bounding_box(image, label, round(x), round(y), round(x+w), round(y+h), color)  \n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Appliquer le code suivant sur l'image de votre choix pour visualiser les détections, en ayant modifié les chemins en fonction de votre configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"/home/paul/libs/darknet/data/dog.jpg\" # input image\n",
    "darknet_dir = \"/home/paul/libs/darknet/\" \n",
    "config_yolo = darknet_dir + \"/cfg/yolov3.cfg\" # the description of the network, layer by layer.\n",
    "class_names = darknet_dir + \"/data/coco.names\" # the list of classes this network can detect\n",
    "weights = \"/home/paul/data/eisti/Image_processing_geometry/yolov3.weights\"\n",
    "\n",
    "# read class names from text file\n",
    "classes = None\n",
    "with open(class_names, 'r') as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "image = cv2.imread(image_path)\n",
    "# read pre-trained model and config file\n",
    "net = cv2.dnn.readNet(weights, config_yolo)\n",
    "boxes, confidences, class_ids = get_detection(image, net)\n",
    "COLORS = np.random.uniform(0, 255, size=(len(classes), 3)) # randomly draw a color for each detection     \n",
    "image = draw_detections(image, class_ids, classes, boxes, COLORS)\n",
    "matplotlib.rcParams['figure.figsize'] = [15, 15]\n",
    "_ = plt.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le script `yolo_on_whole_seq.py` extraiera les détections pour tout les fichiers png présents dans le répertoire indiqué dans son code. \n",
    "Les résultats seront sauvegardés au format json dans le fichier `yolo_detections.json`.\n",
    "\n",
    "Modifiez le script en fonction de votre configuration et utlisez-le pour détecter les objets dans les images.\n",
    "\n",
    "Regardons les résultats pour quelques-unes des images: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "json_file = '/home/paul/data/eisti/Image_processing_geometry/yolo_detections.json'\n",
    "results = json.load(open(json_file)) \n",
    "\n",
    "bottle_class = 39 # in this exercice, we only consider this class\n",
    "image_paths = list(results.keys())\n",
    "fig=plt.figure(figsize=(15, 10))\n",
    "rows, cols = 3, 3\n",
    "for i, image_path in enumerate(image_paths[:9]):\n",
    "    (bbox, conf, class_names) = results[image_path]\n",
    "    bottles = [  box  for  (i, box) in enumerate(bbox)  if class_names[i] == 'bottle' ]\n",
    "    class_ids = [bottle_class for _ in bottles]\n",
    "    image = cv2.imread(image_path)\n",
    "    image = draw_detections(image, class_ids, classes,  bottles, COLORS)\n",
    "    fig.add_subplot(rows, cols, i+1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <a id=\"tracking\"> 3) Tracking : Associer les détections entre les images</a>\n",
    "\n",
    "Dans l'étape précedente, nous avons détecté les bouteilles indépendamment dans chaque image. Mais afin de raisonner géométriquement sur les différentes vues de chaque objet, il nous faut associer les détections de chaque bouteille dans les différentes images dans une même liste. \n",
    "\n",
    "\n",
    "Le script `tracking.py` lit le ficher précédemment créé, associe les détections entre elles et enregistre le résultat dans un fichier `tracking.json`.\n",
    "\n",
    "EXERCICE: Vous devez le compléter en écrivant la fonction qui calcule une ressemblance entre deux détections.\n",
    "\n",
    "Une fois que vous l'avez fait, vous pouvez visualiser les résultats sur avec la cellule suivante. Normalement, chaque objet doit être associé à la même couleur quelle que soit les images.\n",
    "\n",
    "Note: pour visualiser toutes les images, vous pouvez utiliser le script `visualise_tracking.py` qui générera les ~800 images de la séquence dans le répertoire courant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracking_results = json.load(open('/home/paul/Documents/cours/eisti/2020_21/04-Image-Processing-Advanced/labs/tracking.json'))\n",
    "\n",
    "image_paths = list(tracking_results.keys())\n",
    "# randomly draw a color for each track (assuming 10 tracks)\n",
    "COLORS_TRACKING =  np.random.uniform(0, 255, size=(10, 3))\n",
    "track_names = ['track_'+str(i) for i in range(10)]\n",
    "fig=plt.figure(figsize=(15, 10))\n",
    "rows, cols = 3, 3\n",
    "for i, image_path in enumerate(image_paths[:9]): # range(0,len(image_paths), len(image_paths)/16)\n",
    "    track_id_and_bboxes = tracking_results[image_path]\n",
    "    track_ids, bboxes = list(zip(*track_id_and_bboxes))\n",
    "    image = cv2.imread(image_path)\n",
    "    image = draw_detections(image, track_ids, track_names,  bboxes, COLORS_TRACKING)\n",
    "    fig.add_subplot(rows, cols, i+1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <a id=\"sfmo\"> 4) Retrouver les ellipsoides 3D</a>\n",
    "\n",
    "C'est la dernière étape, et la plus importante, il s'agit à présent d'utiliser les détections en 2D de chaque image pour en tirer des ellipsoides en 3D.\n",
    "\n",
    "Tout d'abord, il faut transformer chaque boite englobante en ellipse, encodée par une matrice symétrique 3x3 que nous vectorisons ensuite pour obtenir une vecteur 1x6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from tp_geometry import sfmo\n",
    "from tp_geometry import utils\n",
    "\n",
    "# read the tracking results\n",
    "tracking = json.load(open('tracking.json','r'))\n",
    "n_o = 5 # I put 5 bottles on the table\n",
    "n_f = len(tracking) # number of frames\n",
    "\n",
    "# Construire une ellipse pour chaque boite englobantes et stocker les résultats dans une matrice C\n",
    "C = np.zeros((n_f*3, n_o*3))\n",
    "f = 0 # counting the number of images which are used\n",
    "for (imgPath,dets) in tracking.items():\n",
    "    if len(dets) != n_o:\n",
    "        continue # I ignore images in which less than 5 objects have been detected.\n",
    "    for (trackid, (x, y, w, h)) in dets:\n",
    "        c = sfmo.bbx2ell((x, y, w, h))\n",
    "        if np.abs(c.sum()) <= 0.1 or np.isnan(c).any():\n",
    "            import pdb; pdb.set_trace()\n",
    "        C[f*3:f*3+3, trackid*3:trackid*3+3] = c\n",
    "    f += 1\n",
    "C = C[:3*f] # f is the number of images which are actually used. We truncate the matrix to take only these elements.\n",
    "\n",
    "# Vectoriser les differents composants de cette matrice. \n",
    "# C.a.d. que chaque matrice de conique 3x3 devient un vecteur 6x1\n",
    "# La matrice passe donc de la dimension (n_f*3, n_o*3) à la dimension (nf_*6, n_o)\n",
    "Cadjv_mat = utils.conics_to_vec(C, norm=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La prochaine étape est d'obtenir la position des centres en 3D et les matrices de caméras M à partir des centres en 2D des ellipses. Ouvrez le module `sfmo.py` et compléter le code de la fonction `sfm_from_centers` afin d'implémenter la méthode de Tomasi and Kanade.\n",
    "\n",
    "Note : pour rechargez le module sans redémarrer le notebook, vous pouvez utiliser la librairie `importlib`\n",
    "```\n",
    "import importlib\n",
    "importlib.reload(sfmo)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# structure from motion for the centers of the conics\n",
    "M, S = sfmo.sfm_from_centers(Cadjv_mat)\n",
    "\n",
    "# Add additional orhogonality constraints, this is useful when the measurements are noisy\n",
    "M, S = sfmo.fact_metric_constraint(M,S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La matrice M contient les paramètres de la caméra pour projeter les centres. \n",
    "\n",
    "Nous pouvons l'utiliser pour construire une nouvelle matrice `G` qui liera les paramètres relatifs à la forme des ellipses à ceux des ellipsoides. \n",
    "\n",
    "L'extraction des paramètres liés à la forme des ellipses est effectuée par la fonction `center_ellipses` (il s'agit en fait de centrer les ellipses en fixant leur centres à 0). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a rank 6 reduced matrix, eliminating rows and columns related to translation\n",
    "Gred = sfmo.rebuild_Gred(M)\n",
    "\n",
    "# Remove center from ellipses (it is equivalent to center ellipsoids in the orthographic case)\n",
    "Ccenter = sfmo.center_ellipses(Cadjv_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il reste enfin à calculer la forme des ellipsoides grâce à la méthode de la pseudo inverse à partir du vecteur contenant la forme ellipses et la matrice `Gred`.\n",
    "\n",
    "\n",
    "Comme nous utilisons les ellipses `Ccenter` qui sont centrées, les ellipsoides que nous obtenons sont aussi centrées. \n",
    "Nous ajoutons donc les centres 3D `S` obtenues précédemment avec la fonction `recombine_ellipsoids`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the shape\n",
    "# votre code ici\n",
    "Quadrics_centered = \n",
    "\n",
    "# add the centers\n",
    "Rec  = sfmo.recombine_ellipsoids(Quadrics_centered,S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Affichage des résultats\n",
    "\n",
    "Vous devriez à présent voir 5 ellipsoides allongées dont les positions respectent celle vues sur les images.\n",
    "\n",
    "Encore une fois, Notez que le notebook n'est pas très adapté pour la visualisation 3D. Vous obtiendrez de meilleures visualisation en lançant le code depuis un terminal où matplotlib vous permet de faire pivoter la figure pour l'observer sous différents angles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the 4x4 matrices in structure which explicitly contain the centers, the axis lengths, and the orientation\n",
    "ells = utils.quadrics2ellipsoids(Rec)\n",
    "# Afficher le résultat\n",
    "utils.plot_ellipsoids(ells)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
