Bernouilli <- function(n, p){
echantillon <- rbinom(n, 1, p)
table(echantillon)/n
}
Bernouilli(100, 0.1)
Binomial <- function(n, p){
echantillon <- rbinom(n, 10, p)
Tab_Binomial <- table(echantillon)
hist(Tab_Binomial, breaks = 10, probability = TRUE)
Tab_Binomial/n
}
Binomial(100, 0.5)
Geometrique <- function(n, p){
echantillon <- rgeom(n,p)
Tab_Geo <- table(echantillon)
hist(Tab_Geo, breaks = 10,probability = TRUE)
Tab_Geo/n
}
Geometrique <- function(n, p){
echantillon <- rgeom(n,p)
Tab_Geo <- table(echantillon)
hist(Tab_Geo, breaks = 10,probability = TRUE)
Tab_Geo/n
}
Geometrique <- function(n, p){
echantillon <- rgeom(n,p)
Tab_Geo <- table(echantillon)
hist(Tab_Geo, breaks = 10,probability = TRUE)
Tab_Geo/n
}
Geometrique <- function(n, p){
echantillon <- rgeom(n,p)
Tab_Geo <- table(echantillon)
hist(Tab_Geo, breaks = 10,probability = TRUE)
Tab_Geo/n
}
Geometrique(100, 0.2)
Poisson <- function(n, lamb){
echantillon <- rpois(n,lamb)
Tab_P <- table(echantillon)
hist(Tab_P, probability = TRUE)
Tab_P/n
}
Poisson(100, 10)
Uniforme <- function(n){
echantillon <- floor(runif(n,1,n+1))
Tab_U <- table(echantillon)
hist(Tab_U, breaks = 10, probability = TRUE)
Tab_U/n
}
Uniforme(10)
Uniforme2 <- function(){
echantillon <- runif(100,-1,1)
Tab_U <- table(echantillon)
hist(Tab_U, probability = TRUE)
}
Uniforme2()
Geometrique(100,0.5)
View(Uniforme2)
View(Uniforme)
View(Poisson)
View(Binomial)
View(Poisson)
lapply(ptitanic,class)
library(rpart)
library(rpart.plot)
data(ptitanic)
lapply(ptitanic,class)
ptitanicTree <- rpart(survived~.,data=ptitanic)
View(ptitanicTree)
View(ptitanicTree)
plotcp(ptitanicTree)
ptitanicTree <- rpart(survived~.,data=ptitanic,control=rpart.control(minsplit=5,cp=0))
plotcp(ptitanicTree)
View(ptitanicTree)
ptitanicSimple <- prune(ptitanicTree,cp=0.0047)
plotcp(ptitanicSimple)
ptitanicOptimal <- prune(ptitanicTree,cp=ptitanicTree$cptable[which.min(ptitanicTree$cptable[,4]),1])
plotcp(ptitanicOptimal)
View(ptitanicOptimal)
prp(ptitanicOptimal,extra=1)
prp(ptitanicTree,extra=1)
prp(ptitanicOptimal,extra=1)
table(ptitanic$survived, predict(ptitanicOptimal, ptitanic, type="class"))
View(ptitanic)
prp(ptitanicOptimal,extra=3)
prp(ptitanicOptimal,extra=4)
prp(ptitanicOptimal,extra=5)
prp(ptitanicOptimal,extra=6)
prp(ptitanicOptimal,extra=20)
prp(ptitanicOptimal,extra=10)
prp(ptitanicOptimal,extra=3)
help(rpart)
prp(ptitanicTree,extra=3)
prp(ptitanicOptimal,extra=2)
prp(ptitanicTree,extra=2)
plotcp(ptitanicOptimal)
plotcp(ptitanicTree)
library(randomForest)
myData = read.table("letter-recognition.data",sep=",",col.names = c("lettr","x-box","y-box","width","high" ,"onpix"	,"x-bar"	,"y-bar"	,"x2bar","y2bar"	,"xybar","x2ybr"	,"xy2br","x-ege"	,"xegvy","y-ege"	,"yegvx"))
e
e
e
e
e
e
e
library(randomForest)
letters = read.table("letter-recognition.data",sep=",",col.names = c("lettr","x-box","y-box","width","high" ,"onpix"	,"x-bar"	,"y-bar"	,"x2bar","y2bar"	,"xybar","x2ybr"	,"xy2br","x-ege"	,"xegvy","y-ege"	,"yegvx"))
lapply(letters,class)
n = nrow(letters)
ntrain = floor(2*n/3)
ntest = n - ntrain
indextrain = sample(1:n,ntrain,replace=FALSE)
train = letters[indextrain,]
test = letters[-indextrain,]
setwd("~/Documents/S3/3-IA/Arbres-de-decision-et-forets-aleatoires/letter_recog")
source('~/Documents/S3/3-IA/Arbres-de-decision-et-forets-aleatoires/letter_recog/letter_recog.R')
library(randomForest)
letters = read.table("letter-recognition.data",sep=",",col.names = c("lettr","x-box","y-box","width","high" ,"onpix"	,"x-bar"	,"y-bar"	,"x2bar","y2bar"	,"xybar","x2ybr"	,"xy2br","x-ege"	,"xegvy","y-ege"	,"yegvx"))
lapply(letters,class)
n = nrow(letters)
ntrain = floor(2*n/3)
ntest = n - ntrain
indextrain = sample(1:n,ntrain,replace=FALSE)
train = letters[indextrain,]
test = letters[-indextrain,]
rf <- randomForest(lettr~.,train,ntree = 500, mtry = 4, importance = TRUE) #mtry environ égal à racine(nombres de colonnes)
plot(rf$err.rate[,1],type="l")
varImpPlot(rf)
print(rf)
importance(rf)
prev = predict(rf, train,type="class")
MatConf=table(train$lettr,prev)
print(MatConf)
tb.classes = diag(MatConf)/apply(MatConf,1,sum)
print(tb.classes)
tb = sum(diag(MatConf))/ntrain
print(tb)
prev = predict(rf, test,type="class")
MatConf=table(test$lettr,prev)
print(MatConf)
tb.classes = diag(MatConf)/apply(MatConf,1,sum)
print(tb.classes)
tb = sum(diag(MatConf))/(n - ntrain)
print(tb)
print(tb)
rf <- randomForest(lettr~.,train,ntree = 500, mtry = 4, importance = TRUE) #mtry environ égal à racine(nombres de colonnes)
