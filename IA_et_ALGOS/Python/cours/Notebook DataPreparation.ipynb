{"cells":[{"cell_type":"markdown","id":"clFnDCnmvPjq","metadata":{"id":"clFnDCnmvPjq"},"source":["## <center><h1>**Data preprocessing**</h1>\n","<center> </br>Astrid Jourdan & Peio Loubi√®re & Yannick Le Nir<br/>"]},{"cell_type":"markdown","id":"f56ebe6e","metadata":{"id":"f56ebe6e"},"source":["This tutorial is about data preparation:\n","- import a dataset\n","- checking the type of the variables\n","- checking the distribution of the target variable\n","- creation of a training set and a test set\n","- scaling the variables\n","- encoding the categorical variables"]},{"cell_type":"code","execution_count":1,"id":"77b42727","metadata":{"executionInfo":{"elapsed":1815,"status":"ok","timestamp":1698068623793,"user":{"displayName":"Astrid Jourdan","userId":"07350899265996258726"},"user_tz":-120},"id":"77b42727"},"outputs":[],"source":["# Library importations\n","\n","import math\n","import pandas as pnd\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler"]},{"cell_type":"markdown","id":"ba9c1388","metadata":{"id":"ba9c1388"},"source":["# **Data importation**\n","<br/>\n","The dataset is the multi-spectral values of pixels in 3x3 neighborhoods in a satellite image, and the classification associated with the central pixel in each neighborhood. The target variable is the soil type : <br/><center><i> red soil - cotton crop - grey soil - damp grey soil - soil with vegetation stubble - very damp grey soil </i></center>\n","\n","We display the dimensions and the first three rows to make sure that the file has been read correctly <br/>\n","<br/>"]},{"cell_type":"code","execution_count":null,"id":"5d84c886","metadata":{"id":"5d84c886"},"outputs":[],"source":["# File import\n","dataset = pnd.read_csv(\"Landsat.txt\", delimiter=\" \")\n","\n","# Displays the column and row names\n","print(\"The column names are:\",dataset.columns)\n","print(\"The row names are:\",dataset.index)\n","\n","# Display the dimensions\n","print(\"The dimension of the dataset : \",dataset.shape)\n","\n","# Display the first three rows\n","dataset.head(3)\n","# Display all the dataset\n","#print(dataset.info)\n"]},{"cell_type":"markdown","id":"a6a609bf","metadata":{"id":"a6a609bf"},"source":["<br/> **Variables type**<br/>\n","All the input variables are numerical in this dataset (int64). The target variable is categorical (object).\n","For the preprocessing, we need to get the list of numerical variables (for the normalization) and the list of categorical variables (for the encoding)."]},{"cell_type":"code","execution_count":null,"id":"fe0584ec","metadata":{"id":"fe0584ec"},"outputs":[],"source":["# Displays the variable type\n","print(\"The variable type: \\n\",dataset.dtypes)\n","print(\"\\n\")\n","\n","# Displays the categorical variables\n","ListVarCat = dataset.select_dtypes(include=['object']).columns.tolist()\n","print(\"List of categorical variables: \\n\", ListVarCat)\n","print(\"\\n\")\n","\n","# Display the numeric variables\n","ListVarNum = dataset.select_dtypes(exclude=['object']).columns.tolist()\n","print(\"List of numerical variables: \\n\",ListVarNum)\n"]},{"cell_type":"markdown","id":"3042dd32","metadata":{"id":"3042dd32"},"source":["**Target distribution**\n","<br/>\n","It is necessary to check the distribution of the target variable to ensure that the classes are not unbalanced. We display the table of the numbers of examples in each class as well as a frequency diagram. <br/>\n","<br/>\n","When the target variable is unbalanced two common stategies are used :\n","\n","*   \n","Data reduction : when the number of examples in each class is large enough,\n","\n","*   Data augmentation : when the number of examples in a class is too small, we increase the size of the class by artificially creating new examples in this class (with images we apply transformations: filter, rotation,...)\n"]},{"cell_type":"code","execution_count":null,"id":"5d6518c8","metadata":{"id":"5d6518c8"},"outputs":[],"source":["# Distribution\n","print(\"Distribution : \\n\", dataset['SoilType'].value_counts())\n","print(\"\\n\")\n","\n","# Barplot\n","dataset.groupby('SoilType').size().plot.bar(title=\"Species distribution\", ylabel='Nb of examples')\n","\n","\n","# or with a pie\n","# dataset.groupby('SoilType').size().plot.pie(title=\"Soil type distribution\", ylabel='', autopct='%.2f')\n"]},{"cell_type":"markdown","id":"iNFyggfIHFNn","metadata":{"id":"iNFyggfIHFNn"},"source":["# **Data preprocessing**\n","**Separation of the input variables and the target variable**\n","\n","We use the function *drop* to remove the target variable and create a dataset with the input variables (x) only.\n","We create a vector with the target variable only (y)."]},{"cell_type":"code","execution_count":null,"id":"HFPs1W6PHCAE","metadata":{"id":"HFPs1W6PHCAE"},"outputs":[],"source":["x = dataset.drop(columns=['SoilType']) # Attribute columns\n","y = dataset['SoilType'] # Target column\n","\n","print(\"Inputs: \\n\",x[0:3])\n","print(\"\\n\")\n","print(\"Target : \\n\",y[0:3])\n","print(\"\\n\")\n","\n","n = x.shape[0] # number of examples\n","d = x.shape[1] #number of input variables\n","print(\"The number of examples is\",n)\n","print(\"The number of input variables is\",d)"]},{"cell_type":"markdown","id":"50db606e","metadata":{"id":"50db606e"},"source":["**Training, validation and test subsets**\n","- The training data set is used to find the optimal parameters of the model (the weights for a neural network). Here, we use 70% of the original dataset.\n","- The validation data set is used to fing the best hyperparameters of the model (number of layers, activation functions, optimization algorithm parameters,...). Here we used 15% of the original data set\n","- The test data set is use to measure the performance of the model with examples that were never used to build it. Here, we use 15% of the original dataset.\n"," </br>\n","  </br>\n","The function\n"," </br>\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<i>train_test_split(x, y, train_size = fraction of the training set)</i></br>\n","from library sklearn, splits a dataset (x,y) into two parts. We use it a fisrt time with the original data to create the training dataset and a second time with the rest of the dataset to obtain the validation and test datasets.\n"]},{"cell_type":"code","execution_count":null,"id":"c33a450d","metadata":{"id":"c33a450d"},"outputs":[],"source":["# First Split : 70% training set and 30% fir the test and validation sets\n","xTrain, xRest, yTrain, yRest = train_test_split(x, y, train_size = 0.7, random_state = 42)\n","\n","print(\"Dimensions of the train dataset:\",xTrain.shape)\n","print(\"Dimensions of the remaining dataset:\",xRest.shape)\n","print(\"\\n\")\n","\n","# Second split : 15% for the validation set and 15% for the test set\n","xVal, xTest, yVal, yTest = train_test_split(xRest, yRest, train_size = 0.5, random_state = 42)\n","\n","print(\"Dimensions of the validation dataset:\",xVal.shape)\n","print(\"Dimensions of the test dataset:\",xTest.shape)\n","print(\"\\n\")\n"]},{"cell_type":"markdown","id":"b9f325b0","metadata":{"id":"b9f325b0"},"source":["We can check the target distribution in the training set. The function <i>groupby</i> works with a dataframe. The resulting y (yTrain, yVal and yTest) after splitting is no more a dataframe. In particular, we loose the name of the column. We have to transform it into a dataframe."]},{"cell_type":"code","execution_count":null,"id":"89721e71","metadata":{"id":"89721e71"},"outputs":[],"source":["print(\"y is a vector:\\n\",yTrain[0:5]) # yTrain is a vector and not a dataframe\n","print(\"\\n\")\n","\n","yTrain= pnd.DataFrame(yTrain, columns=[\"SoilType\"]) # Transformation into a dataframe\n","\n","print(\"y is a dataframe:\\n\",yTrain[0:5]) # yTrain is now a dataframe\n","\n","# Verification of the target distribution\n","yTrain.groupby('SoilType').size().plot.bar(title=\"Soil type Train distribution\", ylabel='Nb of examples')\n","\n","\n","yVal= pnd.DataFrame(yVal, columns=[\"SoilType\"])\n","yTest= pnd.DataFrame(yTest, columns=[\"SoilType\"])"]},{"cell_type":"markdown","id":"f0b6a813","metadata":{"id":"f0b6a813"},"source":["**Encoding the target variable**\n"," </br>\n","To encode the categorical variables into binary variables, we use the function </br>\n","</br>\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<i>pnd.get_dummies(X)</i></br>\n","</br>\n","where X is a dataset with only categorical variables.</br>\n","</br>"]},{"cell_type":"code","execution_count":null,"id":"65266230","metadata":{"id":"65266230","outputId":"54c0fcb7-62b9-4e9c-f5fc-d186437a7623"},"outputs":[{"name":"stdout","output_type":"stream","text":["Target variable before binarization \n","       SoilType_cotton crop  SoilType_damp grey soil  SoilType_grey soil  \\\n","1289                     0                        0                   1   \n","1116                     0                        1                   0   \n","583                      1                        0                   0   \n","\n","      SoilType_red soil  SoilType_vegetation stubble  \\\n","1289                  0                            0   \n","1116                  0                            0   \n","583                   0                            0   \n","\n","      SoilType_very damp grey soil  \n","1289                             0  \n","1116                             0  \n","583                              0  \n","\n","\n","Target variable after binarization \n","       SoilType_cotton crop  SoilType_damp grey soil  SoilType_grey soil  \\\n","1289                     0                        0                   1   \n","1116                     0                        1                   0   \n","583                      1                        0                   0   \n","\n","      SoilType_red soil  SoilType_vegetation stubble  \\\n","1289                  0                            0   \n","1116                  0                            0   \n","583                   0                            0   \n","\n","      SoilType_very damp grey soil  \n","1289                             0  \n","1116                             0  \n","583                              0  \n","\n","\n","The number of categories is 6\n"]}],"source":["# Encode the target variable in the training set\n","print(\"Target variable before binarization \\n\",yTrain.head(3)) # Before binarization\n","print(\"\\n\")\n","yTrain=pnd.get_dummies(yTrain)\n","print(\"Target variable after binarization \\n\",yTrain.head(3)) # After binarization\n","print(\"\\n\")\n","\n","# Number of categories\n","p = yTrain.shape[1] #number of classes\n","print(\"The number of categories is\",p)\n","\n","\n","yVal=pnd.get_dummies(yVal)\n","yTest=pnd.get_dummies(yTest)"]},{"cell_type":"markdown","id":"ec7a664a","metadata":{"id":"ec7a664a"},"source":["<br/>\n","Now that the datasets are ready, we will normalize the training set. To each variable X, we apply the transformation (X-mu)/sigma where mu is the mean of X and sigma its standard deviation. The result is a variable such that mu=0 and sigma=1.\n","Then, we apply the same transformation to the test and validation datasets (with mu and sigma from the training set).\n","<br/>\n","<br/>"]},{"cell_type":"code","execution_count":null,"id":"c4cb0f62","metadata":{"id":"c4cb0f62"},"outputs":[],"source":["# Normalization\n","scaler = StandardScaler()\n","\n","\n","# Training set\n","xTrain_scaled = scaler.fit(xTrain)\n","print(\"Training set\")\n","print(\"Means before normalization: \", xTrain_scaled.mean_)\n","print(\"Variance before normalization: \", xTrain_scaled.var_)\n","print(\"\\n\")\n","\n","\n","# Normalization of the training set\n","xTrain_scaled = scaler.transform(xTrain)\n","print(\"Means after normalization: \",xTrain_scaled.mean(axis=0))\n","print(\"Variance after normalization: \",xTrain_scaled.var(axis=0))\n","print(\"\\n\")\n","xTrain_scaled= pnd.DataFrame(xTrain_scaled, columns=[xTrain.columns.tolist()]) # transform into a dataframe with the column names of xTrain\n","xTrain_scaled.head()\n"]},{"cell_type":"code","execution_count":null,"id":"7ehKhhM6EL0j","metadata":{"id":"7ehKhhM6EL0j"},"outputs":[],"source":["# Transformation of the validation set\n","xVal_scaled = scaler.transform(xVal)\n","print(\"Validation set\")\n","print(\"Means after normalization: \",xVal_scaled.mean(axis = 0))\n","print(\"Variance after normalization: \",xVal_scaled.var(axis = 0))\n","print(\"\\n\")\n","xVal_scaled= pnd.DataFrame(xVal_scaled, columns=[xTrain.columns.tolist()]) # transform into a dataframe with the column names of xTrain\n","\n"]},{"cell_type":"code","execution_count":null,"id":"gBM4tYOtEODm","metadata":{"id":"gBM4tYOtEODm"},"outputs":[],"source":["\n","\n","# Transformation of the test set\n","xTest_scaled = scaler.transform(xTest)\n","print(\"Test set\")\n","print(\"Means after normalization: \",xTest_scaled.mean(axis = 0))\n","print(\"Variance after normalization: \",xTest_scaled.var(axis = 0))\n","print(\"\\n\")\n","xTest_scaled= pnd.DataFrame(xTest_scaled, columns=[xTrain.columns.tolist()]) # transform into a dataframe with the column names of xTrain\n","\n"]},{"cell_type":"markdown","id":"fb3805ee","metadata":{"id":"fb3805ee"},"source":[" <center>\n","<h1>EXERCICE</h1>\n","</center>\n","\n","The dataset <i>Drug_Consumption.csv</i> contains records for respondents. For each respondent, we known: <br/>\n","<br/>\n","<ul>\n","<li>Personality measurements which include NEO-FFI-R (neuroticism, extraversion, openness to experience, agreeableness, and conscientiousness), BIS-11 (impulsivity), and ImpSS (sensation seeking). </li>\n","<li>Personal information which include level of education, age, gender, country of residence and ethnicity.</li>\n","<li>Cocaine consumption.</li>\n","<br/>\n","\n","<ol>\n","<li>How many respondents have been recorded? Display the 3th rows of the dataset.</li>\n","<li>How many variables are in the dataset? What is their type?</li>\n","<li>What is the target variable? What is its type? Display its distribution.</li>\n","<li>Split the dataset into a training, validation and test datasets (80%-10%-10%).</li>\n","<li>Encode the target variable.</li>\n","<li>Extract the numeric variables and normalize them.</li>\n","<li>Extract the categorical variables and encode them.</li>\n","<li>Build a new x dataset with the encoded input variables and the standardized variables.</li>\n","<ol/>"]},{"cell_type":"code","execution_count":null,"id":"cccb2302","metadata":{"id":"cccb2302"},"outputs":[],"source":["## QUESTION 1\n","##\n","## Your code here\n","##\n","## Warning The delimiter in the data file is \";\""]},{"cell_type":"code","execution_count":null,"id":"312ec78c","metadata":{"id":"312ec78c"},"outputs":[],"source":["## QUESTION 2\n","##\n","## Your code here\n","##"]},{"cell_type":"code","execution_count":null,"id":"a7b9122d","metadata":{"id":"a7b9122d"},"outputs":[],"source":["## QUESTION 3\n","##\n","## Your code here\n","##"]},{"cell_type":"code","execution_count":null,"id":"400c3f66","metadata":{"id":"400c3f66"},"outputs":[],"source":["## QUESTION 4\n","##\n","## Your code here\n","##"]},{"cell_type":"code","execution_count":null,"id":"1f008648","metadata":{"id":"1f008648"},"outputs":[],"source":["## QUESTION 5\n","##\n","## Your code here\n","##"]},{"cell_type":"code","execution_count":null,"id":"363b9c79","metadata":{"id":"363b9c79"},"outputs":[],"source":["## QUESTION 6\n","##\n","## Complet the code\n","##\n","\n","# List of numerical variables\n","numeric_list = \n","# Standardization\n","numeric_xTrain= # training set with the numerical variables\n","numeric_xTrain_scaled = scaler.fit(numeric_xTrain)\n","numeric_xTrain_scaled = scaler.transform(numeric_xTrain)\n","print(\"Training set\")\n","print(\"Means after normalization: \",numeric_xTrain_scaled.mean(axis = 0))\n","print(\"Variance after normalization: \",numeric_xTrain_scaled.var(axis = 0))\n","print(\"\\n\")\n","\n","# Transformation into a dataframe\n","numeric_xTrain_scaled=pnd.DataFrame(numeric_xTrain_scaled, index=xTrain.index,columns=numeric_list)"]},{"cell_type":"code","execution_count":null,"id":"3e6a3816","metadata":{"id":"3e6a3816"},"outputs":[],"source":["## QUESTION 6 (continued)\n","##\n","## Your code here\n","##"]},{"cell_type":"markdown","id":"80951fc0","metadata":{"id":"80951fc0"},"source":["To encode the categorical variables into binary variables, we use the function </br>\n","</br>\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<i>pnd.get_dummies(X)</i></br>\n","</br>\n","where X is a dataset with only categorical variables.</br>\n","</br>\n","We process in 3 steps : </br>\n","<ol>\n","<li>We extract the categorical variables from the dataset.</li>\n","<li>We encode the categorical variables.</li>\n","<li>We rebuilt the dataset by concatening the encoded variables, the numerical variables and the target variable. </li>\n","<ol/>"]},{"cell_type":"code","execution_count":null,"id":"9e4e3053","metadata":{"id":"9e4e3053"},"outputs":[],"source":["## QUESTION 7\n","##\n","## Complet the code\n","##\n","# List of the categorical variables\n","cat_list= # List of categorical variables\n","print(cat_list)\n","print(\"\\n\")\n","\n","# Encode the categorical variables in the training set\n","xTrain_cat=xTrain[cat_list[:-1]] # xTrain set with the categorical variables except the target Cocaine\n","print(xTrain_cat.head(3)) # Display the categorical variables before binarization\n","print(\"\\n\")\n","xTrain_cat_encoded=pnd.get_dummies(xTrain_cat)\n","print(xTrain_cat_encoded.head(3)) # Display the categorical variables after binarization\n","\n","# Encode the categorical variables in the test set\n"]},{"cell_type":"markdown","id":"ee2cd915","metadata":{"id":"ee2cd915"},"source":["To concatenate dataframes, data1, data2, ..., we use the function\n","</br>\n","</br>\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<i>pnd.concat([data1,data2,...],axis=1)</i></br>\n","</br>\n","where <i>axis</i> is the axis to concatenate along (axis=1 for columns and axis=0 for rows)\n"]},{"cell_type":"code","execution_count":null,"id":"cda412ab","metadata":{"id":"cda412ab"},"outputs":[],"source":["## QUESTION 8\n","##\n","## Complet the code\n","##\n","\n","# Concatenate the numerical variables and the encoded variables for the training set\n","xTrain_ready=pnd.concat([numeric_xTrain_scaled,xTrain_cat_encoded],axis=1) # Concatenates tables by columns (axis=1)\n","print(\"The number of variables is the new dataset is\",xTrain_ready.shape[1])\n","\n","# Concatenate the numerical variables and the encoded variables for the test set\n","\n","\n","xTrain_ready.head(3)"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"}},"nbformat":4,"nbformat_minor":5}
