{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"border: 2px solid black; padding: 15px; border-radius: 12px;\" align='center'>Cours IA et Applications</h1>    \n",
    "\n",
    "<h2 align='center'> TD: Natural Language Processing </h2>\n",
    "\n",
    "<h3 align='center'> Jordy Palafox </h3>\n",
    "\n",
    "<img src=\"cytech.png\" \n",
    "        alt=\"Picture\" \n",
    "        style=\"display: block; margin: 0 auto\" />\n",
    "\n",
    "# Exercice 1\n",
    "\n",
    "Le but de cet exercice est d'étudier des tweets et plus précisément d'effectuer des recherches sur des tweets à partir d'expressions régulières (regex)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1 \n",
    "Recherchons les noms d'utilisateurs dans un lot de tweets d'exemples tirés de NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     /home/cytech/nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import twitter_samples\n",
    "import re\n",
    "nltk.download('twitter_samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['negative_tweets.json', 'positive_tweets.json', 'tweets.20150430-223406.json']\n",
      "-------------------------\n",
      "['RT @KirkKus: Indirect cost of the UK being in the EU is estimated to be costing Britain £170 billion per year! #BetterOffOut #UKIP', 'VIDEO: Sturgeon on post-election deals http://t.co/BTJwrpbmOY', 'RT @LabourEoin: The economy was growing 3 times faster on the day David Cameron became Prime Minister than it is today.. #BBCqt http://t.co…', 'RT @GregLauder: the UKIP east lothian candidate looks about 16 and still has an msn addy http://t.co/7eIU0c5Fm1', \"RT @thesundaypeople: UKIP's housing spokesman rakes in £800k in housing benefit from migrants.  http://t.co/GVwb9Rcb4w http://t.co/c1AZxcLh…\", 'RT @Nigel_Farage: Make sure you tune in to #AskNigelFarage tonight on BBC 1 at 22:50! #UKIP http://t.co/ogHSc2Rsr2', 'RT @joannetallis: Ed Milliband is an embarrassment. Would you want him representing the UK?!  #bbcqt vote @Conservatives', \"RT @abstex: The FT is backing the Tories. On an unrelated note, here's a photo of FT leader writer Jonathan Ford (next to Boris) http://t.c…\", \"RT @NivenJ1: “@George_Osborne: Ed Miliband proved tonight why he's not up to the job” Tbf you've spent 5 years doing that you salivating do…\", \"LOLZ to Trickle Down Wealth. It's never trickling past their own wallets. Greed always wins $$$ for the greedy.  https://t.co/X7deoPbS97\"]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "20000\n",
      "List : [('@Nigel_Farage', 673), ('@UKIP', 672), ('@NicolaSturgeon', 633), ('@Tommy_Colc', 632), ('@Ed_Miliband', 627), ('@LabourEoin', 295), ('@JimForScotland', 272), ('@theSNP', 255), ('@UKLabour', 227), ('@OwenJones84', 178)]\n"
     ]
    }
   ],
   "source": [
    "lst_fileids = twitter_samples.fileids()\n",
    "print(lst_fileids)\n",
    "print('-------------------------')\n",
    "\n",
    "strings = twitter_samples.strings('tweets.20150430-223406.json')\n",
    "print(strings[:10])\n",
    "print('--'*50)\n",
    "\n",
    "print(len(strings))\n",
    "dict_Users = {}\n",
    "for string in strings:\n",
    "    x=re.findall(\"@[A-Za-z0-9_]{4,15}\",string)\n",
    "    if (x!=[]):\n",
    "      for usr in x:\n",
    "        if usr not in dict_Users.keys():\n",
    "          dict_Users[usr] = 1\n",
    "        else:\n",
    "          dict_Users[usr] += 1\n",
    "          \n",
    "dict_users_sorted = {k: v for k, v in sorted(dict_Users.items(), \\\n",
    "     key=lambda item: item[1], reverse=True)}\n",
    "users_v1 = list(dict_Users.keys())\n",
    "#On affiche les 10 noms d'utilisateurs  les plus populaires\n",
    "print(\"List :\",list(dict_users_sorted.items())[:10])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "Recherchons les hastags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List : [('#bbcqt', 2106), ('#AskNigelFarage', 1168), ('#UKIP', 951), ('#GE2015', 626), ('#SNP', 616), ('#BBCQT', 240), ('#AskFarage', 210), ('#BBCqt', 202), ('#Labour', 195), ('#VoteSNP', 188)]\n",
      "List : [('#bbcqt', 2095), ('#AskNigelFarage', 1168), ('#UKIP', 951), ('#GE2015', 626), ('#SNP', 615), ('#BBCQT', 238), ('#AskFarage', 210), ('#BBCqt', 202), ('#Labour', 194), ('#VoteSNP', 188)]\n",
      "#Labour-\n",
      "#bbcqt)\n",
      "#TrapHou$e\n",
      "#RedTories&gt\n",
      "#BBCQT)\n",
      "#出会い\n",
      "#セフレ\n",
      "#メル友\n",
      "#Clegg+some\n",
      "#Ukip)\n",
      "#LabourUnbelievable-Miliband\n",
      "#SNP-support\n",
      "#無料\n",
      "#招待\n",
      "#Miliband’s\n",
      "#BBC]\n",
      "#英国総選挙\n",
      "#هوشه_المطار\n",
      "#Anti-Austerity\n",
      "#BetterThanFarageáTrois\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dict_Hashtags = {}\n",
    "for string in strings:\n",
    "    x=re.findall(\"#[A-Za-z0-9_]+\",string)\n",
    "    if (x!=[]):\n",
    "      for htg in x:\n",
    "        if htg not in dict_Hashtags.keys():\n",
    "          dict_Hashtags[htg] = 1\n",
    "        else:\n",
    "          dict_Hashtags[htg] += 1\n",
    "          \n",
    "dict_Hashtags_sorted = {k: v for k, v in sorted(dict_Hashtags.items(), key=lambda item: item[1], reverse=True)}\n",
    "hashtags_v1 = list(dict_Hashtags.keys())\n",
    "#On affiche les 10 hashtags les plus populaires\n",
    "print(\"List :\",list(dict_Hashtags_sorted.items())[:10])\n",
    "\n",
    "dict_Hashtags = {}\n",
    "for string in strings:\n",
    "    x=re.findall(\"#[^#/\\s\\.:!,;\\?'\\\"\\”…]+\",string)\n",
    "    if (x!=[]):\n",
    "      for htg in x:\n",
    "        if htg not in dict_Hashtags.keys():\n",
    "          dict_Hashtags[htg] = 1\n",
    "        else:\n",
    "          dict_Hashtags[htg] += 1\n",
    "          \n",
    "dict_Hashtags_sorted = {k: v for k, v in sorted(dict_Hashtags.items(), key=lambda item: item[1], reverse=True)}\n",
    "hashtags_v2 = list(dict_Hashtags.keys())\n",
    "\n",
    "#On affiche les 10 hashtags les plus populaires\n",
    "print(\"List :\",list(dict_Hashtags_sorted.items())[:10])\n",
    "\n",
    "for hshtg in hashtags_v2:\n",
    "  if (hshtg not in hashtags_v1):\n",
    "      print(hshtg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "Recherche dans des URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Second regular expression :\n",
      "Third regular expression :\n",
      "(0, 33) https://www.regextester.com/94502\n",
      "(34, 97) https://www.boursier.com/actions/cours/edf-FR0010242511,FR.html\n",
      "(98, 131) https://en.wikipedia.org/wiki/URL\n",
      "(139, 236) https://www.apec.fr/candidat/recherche-emploi.html/emploi?salaires=101843&secteursActivite=101767\n",
      "(237, 263) https://journal.lemonde.fr\n",
      "(264, 506) https://www.hostinger.fr/tutoriels/quest-ce-quun-nom-de-domaine/?ppc_campaign=google_search_generic_hosting_all&bidkw=defaultkeyword&lo=9056612&gclid=Cj0KCQiAy4eNBhCaARIsAFDVtI0ixlDFnkr5RoFq36lVwRX-p4q5kBy9mCX1lVQtStbZcXxC9Dv1Li8aAirSEALw_wcB\n",
      "(507, 549) https://dbpedia.org/page/French_Revolution\n",
      "(550, 582) https://mail.google.com/mail/u/0\n",
      "(590, 628) https://partage.cyu.fr/?loginOp=logout\n",
      "(629, 702) https://www.linkedin.com/feed/?trk=guest_homepage-basic_nav-header-signin\n",
      "(703, 746) https://www.w3.org/1999/02/22-rdf-syntax-ns\n",
      "(756, 835) https://stackoverflow.com/questions/1547899/which-characters-make-a-url-invalid\n",
      "Forth regular expression :\n",
      "(0, 33) https://www.regextester.com/94502\n",
      "(34, 97) https://www.boursier.com/actions/cours/edf-FR0010242511,FR.html\n",
      "(98, 138) https://en.wikipedia.org/wiki/URL#Syntax\n",
      "(139, 236) https://www.apec.fr/candidat/recherche-emploi.html/emploi?salaires=101843&secteursActivite=101767\n",
      "(237, 263) https://journal.lemonde.fr\n",
      "(264, 506) https://www.hostinger.fr/tutoriels/quest-ce-quun-nom-de-domaine/?ppc_campaign=google_search_generic_hosting_all&bidkw=defaultkeyword&lo=9056612&gclid=Cj0KCQiAy4eNBhCaARIsAFDVtI0ixlDFnkr5RoFq36lVwRX-p4q5kBy9mCX1lVQtStbZcXxC9Dv1Li8aAirSEALw_wcB\n",
      "(507, 549) https://dbpedia.org/page/French_Revolution\n",
      "(550, 589) https://mail.google.com/mail/u/0/#inbox\n",
      "(590, 628) https://partage.cyu.fr/?loginOp=logout\n",
      "(629, 702) https://www.linkedin.com/feed/?trk=guest_homepage-basic_nav-header-signin\n",
      "(703, 755) https://www.w3.org/1999/02/22-rdf-syntax-ns#Property\n",
      "(756, 835) https://stackoverflow.com/questions/1547899/which-characters-make-a-url-invalid\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pn\n",
    "\n",
    "df_urls = pn.read_csv('url4reg.csv')\n",
    "\n",
    "txt = ''\n",
    "for num in range(len(df_urls)):\n",
    "  txt = txt+df_urls.loc[num,'url']+' '\n",
    "\n",
    "'''reg1='(https?://)(www\\.)?([a-zA-Z0-9_]+\\.)+[a-zA-Z]+'\n",
    "\n",
    "a=re.finditer(r'(https?://)(www\\.)?([a-zA-Z0-9_]+\\.)+[a-zA-Z]+',txt)\n",
    "for m in a:\n",
    "    print(m)'''\n",
    "\t\n",
    "#print('Second regular expression :')\n",
    "\t\n",
    "reg2=r'(https?://)(www\\.)?([a-zA-Z0-9_]+\\.)+[a-zA-Z]+(/[^/#?\\s]+)*'\n",
    "\n",
    "'''b=re.finditer(reg2,txt)\n",
    "for m in b:\n",
    "    print(m.span(), m.group())\n",
    "'''\n",
    "print('Third regular expression :')\n",
    "reg3=reg2+'(/?\\?[^/#?\\s]+)*'\n",
    "c=re.finditer(reg3,txt)\n",
    "for m in c:\n",
    "    print(m.span(), m.group())\n",
    "    \n",
    "print('Forth regular expression :')\n",
    "reg4=reg3+'(/?\\#[^/#?\\s]+)*'\n",
    "d=re.finditer(reg4,txt)\n",
    "for m in d:\n",
    "    print(m.span(), m.group())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercice 2\n",
    "On va réaliser une analyse lexicale en suivant les étapes suivantes : \n",
    "+ tokenizer et nettoyer un texte (autrement dit chaque mot, \n",
    "             lettre ou groupe de mots devient un token et on supprime les stopwords)\n",
    "+ associer à chaque mot son taf POS puis lister les mots par tag\n",
    "+ afficher la description d'un tag ou d'une famille de tags,\n",
    "+ lemmatiser les mots du texte (donc transformer les mots en leur racine).             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/cytech/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/cytech/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package tagsets to /home/cytech/nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/cytech/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from nltk.tag import pos_tag\n",
    "import re\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('tagsets')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Manipulation de base avec NLTK/wordnet\n",
    "\n",
    "#Détection de la langue avec les stopwords\n",
    "\n",
    "#Lecture du fichier\n",
    "\n",
    "file = open('theguardian.txt', 'r')\n",
    "read_file = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'When Bernie Sanders came out to face his supporters in his home town of Burlington, Vermont, on Tuesday night he projected no signs of self-doubt. \"We are going to defeat Trump because we are putting together an unprecedented grassroots multi-generational, multi-racial movement\" he boomed, thumping the podium.Even by that point in the night, though, the Super Tuesday results were pointing to a movement coalescing not around Sanders but around his resurgent opponent Joe Biden. By the morning after, the hangover had worsened-data from several states suggested that Sanders\\' core support had softened a little and had doggedly failed to expand. Vermont, the bastion of the Sanders revolution since he won his first mayoral election in Burlington exactly 39 years ago to the day-3 March 1981-itself told a story.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'when bernie sanders came out to face his supporters in his home town of burlington vermont on tuesday night he projected no signs of selfdoubt we are going to defeat trump because we are putting together an unprecedented grassroots multigenerational multiracial movement he boomed thumping the podiumeven by that point in the night though the super tuesday results were pointing to a movement coalescing not around sanders but around his resurgent opponent joe biden by the morning after the hangover had worseneddata from several states suggested that sanders core support had softened a little and had doggedly failed to expand vermont the bastion of the sanders revolution since he won his first mayoral election in burlington exactly  years ago to the day march itself told a story'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Ici le nettoyage du texte(plus exactement la préparation aux traitements qui\n",
    "#vont suivre) consiste à le mettre en minuscule puis supprimer\n",
    "#les signes de ponctuation et les chiffres.\n",
    "\n",
    "text = read_file.lower()\n",
    "new_txt = re.sub(r\"[0-9,-\\.\\\"\\']+\",'',text)\n",
    "\n",
    "new_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['when',\n",
       " 'bernie',\n",
       " 'sanders',\n",
       " 'came',\n",
       " 'out',\n",
       " 'to',\n",
       " 'face',\n",
       " 'his',\n",
       " 'supporters',\n",
       " 'in',\n",
       " 'his',\n",
       " 'home',\n",
       " 'town',\n",
       " 'of',\n",
       " 'burlington',\n",
       " 'vermont',\n",
       " 'on',\n",
       " 'tuesday',\n",
       " 'night',\n",
       " 'he',\n",
       " 'projected',\n",
       " 'no',\n",
       " 'signs',\n",
       " 'of',\n",
       " 'selfdoubt',\n",
       " 'we',\n",
       " 'are',\n",
       " 'going',\n",
       " 'to',\n",
       " 'defeat',\n",
       " 'trump',\n",
       " 'because',\n",
       " 'we',\n",
       " 'are',\n",
       " 'putting',\n",
       " 'together',\n",
       " 'an',\n",
       " 'unprecedented',\n",
       " 'grassroots',\n",
       " 'multigenerational',\n",
       " 'multiracial',\n",
       " 'movement',\n",
       " 'he',\n",
       " 'boomed',\n",
       " 'thumping',\n",
       " 'the',\n",
       " 'podiumeven',\n",
       " 'by',\n",
       " 'that',\n",
       " 'point',\n",
       " 'in',\n",
       " 'the',\n",
       " 'night',\n",
       " 'though',\n",
       " 'the',\n",
       " 'super',\n",
       " 'tuesday',\n",
       " 'results',\n",
       " 'were',\n",
       " 'pointing',\n",
       " 'to',\n",
       " 'a',\n",
       " 'movement',\n",
       " 'coalescing',\n",
       " 'not',\n",
       " 'around',\n",
       " 'sanders',\n",
       " 'but',\n",
       " 'around',\n",
       " 'his',\n",
       " 'resurgent',\n",
       " 'opponent',\n",
       " 'joe',\n",
       " 'biden',\n",
       " 'by',\n",
       " 'the',\n",
       " 'morning',\n",
       " 'after',\n",
       " 'the',\n",
       " 'hangover',\n",
       " 'had',\n",
       " 'worseneddata',\n",
       " 'from',\n",
       " 'several',\n",
       " 'states',\n",
       " 'suggested',\n",
       " 'that',\n",
       " 'sanders',\n",
       " 'core',\n",
       " 'support',\n",
       " 'had',\n",
       " 'softened',\n",
       " 'a',\n",
       " 'little',\n",
       " 'and',\n",
       " 'had',\n",
       " 'doggedly',\n",
       " 'failed',\n",
       " 'to',\n",
       " 'expand',\n",
       " 'vermont',\n",
       " 'the',\n",
       " 'bastion',\n",
       " 'of',\n",
       " 'the',\n",
       " 'sanders',\n",
       " 'revolution',\n",
       " 'since',\n",
       " 'he',\n",
       " 'won',\n",
       " 'his',\n",
       " 'first',\n",
       " 'mayoral',\n",
       " 'election',\n",
       " 'in',\n",
       " 'burlington',\n",
       " 'exactly',\n",
       " 'years',\n",
       " 'ago',\n",
       " 'to',\n",
       " 'the',\n",
       " 'day',\n",
       " 'march',\n",
       " 'itself',\n",
       " 'told',\n",
       " 'a',\n",
       " 'story']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tokenisation\n",
    "\n",
    "words = word_tokenize(new_txt)\n",
    "\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('when', 'WRB'),\n",
       " ('bernie', 'NN'),\n",
       " ('sanders', 'NNS'),\n",
       " ('came', 'VBD'),\n",
       " ('out', 'RP'),\n",
       " ('to', 'TO'),\n",
       " ('face', 'VB'),\n",
       " ('his', 'PRP$'),\n",
       " ('supporters', 'NNS'),\n",
       " ('in', 'IN'),\n",
       " ('his', 'PRP$'),\n",
       " ('home', 'NN'),\n",
       " ('town', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('burlington', 'NN'),\n",
       " ('vermont', 'NN'),\n",
       " ('on', 'IN'),\n",
       " ('tuesday', 'JJ'),\n",
       " ('night', 'NN'),\n",
       " ('he', 'PRP'),\n",
       " ('projected', 'VBD'),\n",
       " ('no', 'DT'),\n",
       " ('signs', 'NNS'),\n",
       " ('of', 'IN'),\n",
       " ('selfdoubt', 'NN'),\n",
       " ('we', 'PRP'),\n",
       " ('are', 'VBP'),\n",
       " ('going', 'VBG'),\n",
       " ('to', 'TO'),\n",
       " ('defeat', 'VB'),\n",
       " ('trump', 'NN'),\n",
       " ('because', 'IN'),\n",
       " ('we', 'PRP'),\n",
       " ('are', 'VBP'),\n",
       " ('putting', 'VBG'),\n",
       " ('together', 'RB'),\n",
       " ('an', 'DT'),\n",
       " ('unprecedented', 'JJ'),\n",
       " ('grassroots', 'NNS'),\n",
       " ('multigenerational', 'JJ'),\n",
       " ('multiracial', 'JJ'),\n",
       " ('movement', 'NN'),\n",
       " ('he', 'PRP'),\n",
       " ('boomed', 'VBD'),\n",
       " ('thumping', 'VBG'),\n",
       " ('the', 'DT'),\n",
       " ('podiumeven', 'NN'),\n",
       " ('by', 'IN'),\n",
       " ('that', 'DT'),\n",
       " ('point', 'NN'),\n",
       " ('in', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('night', 'NN'),\n",
       " ('though', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('super', 'NN'),\n",
       " ('tuesday', 'NN'),\n",
       " ('results', 'NNS'),\n",
       " ('were', 'VBD'),\n",
       " ('pointing', 'VBG'),\n",
       " ('to', 'TO'),\n",
       " ('a', 'DT'),\n",
       " ('movement', 'NN'),\n",
       " ('coalescing', 'VBG'),\n",
       " ('not', 'RB'),\n",
       " ('around', 'IN'),\n",
       " ('sanders', 'NNS'),\n",
       " ('but', 'CC'),\n",
       " ('around', 'IN'),\n",
       " ('his', 'PRP$'),\n",
       " ('resurgent', 'NN'),\n",
       " ('opponent', 'NN'),\n",
       " ('joe', 'NN'),\n",
       " ('biden', 'NN'),\n",
       " ('by', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('morning', 'NN'),\n",
       " ('after', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('hangover', 'NN'),\n",
       " ('had', 'VBD'),\n",
       " ('worseneddata', 'VBN'),\n",
       " ('from', 'IN'),\n",
       " ('several', 'JJ'),\n",
       " ('states', 'NNS'),\n",
       " ('suggested', 'VBD'),\n",
       " ('that', 'IN'),\n",
       " ('sanders', 'NNS'),\n",
       " ('core', 'VBP'),\n",
       " ('support', 'NN'),\n",
       " ('had', 'VBD'),\n",
       " ('softened', 'VBN'),\n",
       " ('a', 'DT'),\n",
       " ('little', 'JJ'),\n",
       " ('and', 'CC'),\n",
       " ('had', 'VBD'),\n",
       " ('doggedly', 'RB'),\n",
       " ('failed', 'VBN'),\n",
       " ('to', 'TO'),\n",
       " ('expand', 'VB'),\n",
       " ('vermont', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('bastion', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('sanders', 'NNS'),\n",
       " ('revolution', 'NN'),\n",
       " ('since', 'IN'),\n",
       " ('he', 'PRP'),\n",
       " ('won', 'VBD'),\n",
       " ('his', 'PRP$'),\n",
       " ('first', 'JJ'),\n",
       " ('mayoral', 'JJ'),\n",
       " ('election', 'NN'),\n",
       " ('in', 'IN'),\n",
       " ('burlington', 'NN'),\n",
       " ('exactly', 'RB'),\n",
       " ('years', 'NNS'),\n",
       " ('ago', 'RB'),\n",
       " ('to', 'TO'),\n",
       " ('the', 'DT'),\n",
       " ('day', 'NN'),\n",
       " ('march', 'VBD'),\n",
       " ('itself', 'PRP'),\n",
       " ('told', 'VBD'),\n",
       " ('a', 'DT'),\n",
       " ('story', 'NN')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#POS tags\n",
    "\n",
    "list_tags=pos_tag(words)\n",
    "\n",
    "list_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'WRB': ['when'],\n",
       " 'NN': ['bernie',\n",
       "  'home',\n",
       "  'town',\n",
       "  'burlington',\n",
       "  'vermont',\n",
       "  'night',\n",
       "  'selfdoubt',\n",
       "  'trump',\n",
       "  'movement',\n",
       "  'podiumeven',\n",
       "  'point',\n",
       "  'super',\n",
       "  'tuesday',\n",
       "  'resurgent',\n",
       "  'opponent',\n",
       "  'joe',\n",
       "  'biden',\n",
       "  'morning',\n",
       "  'hangover',\n",
       "  'support',\n",
       "  'bastion',\n",
       "  'revolution',\n",
       "  'election',\n",
       "  'day',\n",
       "  'story'],\n",
       " 'NNS': ['sanders',\n",
       "  'supporters',\n",
       "  'signs',\n",
       "  'grassroots',\n",
       "  'results',\n",
       "  'states',\n",
       "  'years'],\n",
       " 'VBD': ['came',\n",
       "  'projected',\n",
       "  'boomed',\n",
       "  'were',\n",
       "  'had',\n",
       "  'suggested',\n",
       "  'won',\n",
       "  'march',\n",
       "  'told'],\n",
       " 'RP': ['out'],\n",
       " 'TO': ['to'],\n",
       " 'VB': ['face', 'defeat', 'expand'],\n",
       " 'PRP$': ['his'],\n",
       " 'IN': ['in',\n",
       "  'of',\n",
       "  'on',\n",
       "  'because',\n",
       "  'by',\n",
       "  'though',\n",
       "  'around',\n",
       "  'after',\n",
       "  'from',\n",
       "  'that',\n",
       "  'vermont',\n",
       "  'since'],\n",
       " 'JJ': ['tuesday',\n",
       "  'unprecedented',\n",
       "  'multigenerational',\n",
       "  'multiracial',\n",
       "  'several',\n",
       "  'little',\n",
       "  'first',\n",
       "  'mayoral'],\n",
       " 'PRP': ['he', 'we', 'itself'],\n",
       " 'DT': ['no', 'an', 'the', 'that', 'a'],\n",
       " 'VBP': ['are', 'core'],\n",
       " 'VBG': ['going', 'putting', 'thumping', 'pointing', 'coalescing'],\n",
       " 'RB': ['together', 'not', 'doggedly', 'exactly', 'ago'],\n",
       " 'CC': ['but', 'and'],\n",
       " 'VBN': ['worseneddata', 'softened', 'failed']}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#listing the words of each POS tag\n",
    "dict_freq = {}\n",
    "for e in list_tags:\n",
    "  e_tag=e[1]\n",
    "  if e_tag not in dict_freq.keys():\n",
    "    dict_freq[e_tag] = [e[0]]\n",
    "  else:\n",
    "    if (e[0] not in dict_freq[e_tag]):\n",
    "      dict_freq[e_tag].append(e[0])\n",
    "\t  \n",
    "dict_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "NNP: noun, proper, singular\n",
      "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
      "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
      "    Shannon A.K.C. Meltex Liverpool ...\n",
      "NNPS: noun, proper, plural\n",
      "    Americans Americas Amharas Amityvilles Amusements Anarcho-Syndicalists\n",
      "    Andalusians Andes Andruses Angels Animals Anthony Antilles Antiques\n",
      "    Apache Apaches Apocrypha ...\n",
      "NNS: noun, common, plural\n",
      "    undergraduates scotches bric-a-brac products bodyguards facets coasts\n",
      "    divestitures storehouses designs clubs fragrances averages\n",
      "    subjectivists apprehensions muses factory-jobs ...\n",
      "MD: modal auxiliary\n",
      "    can cannot could couldn't dare may might must need ought shall should\n",
      "    shouldn't will would\n"
     ]
    }
   ],
   "source": [
    "#Information au sujet des tags\n",
    "nltk.help.upenn_tagset('NN.*')\n",
    "nltk.help.upenn_tagset('MD')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when when\n",
      "bernie bernie\n",
      "sanders sander\n",
      "came come\n",
      "out out\n",
      "to to\n",
      "face face\n",
      "his his\n",
      "supporters supporter\n",
      "in in\n",
      "his his\n",
      "home home\n",
      "town town\n",
      "of of\n",
      "burlington burlington\n",
      "vermont vermont\n",
      "on on\n",
      "tuesday tuesday\n",
      "night night\n",
      "he he\n",
      "projected project\n",
      "no no\n",
      "signs sign\n",
      "of of\n",
      "selfdoubt selfdoubt\n",
      "we we\n",
      "are be\n",
      "going go\n",
      "to to\n",
      "defeat defeat\n",
      "trump trump\n",
      "because because\n",
      "we we\n",
      "are be\n",
      "putting put\n",
      "together together\n",
      "an an\n",
      "unprecedented unprecedented\n",
      "grassroots grassroots\n",
      "multigenerational multigenerational\n",
      "multiracial multiracial\n",
      "movement movement\n",
      "he he\n",
      "boomed boom\n",
      "thumping thump\n",
      "the the\n",
      "podiumeven podiumeven\n",
      "by by\n",
      "that that\n",
      "point point\n",
      "in in\n",
      "the the\n",
      "night night\n",
      "though though\n",
      "the the\n",
      "super super\n",
      "tuesday tuesday\n",
      "results result\n",
      "were be\n",
      "pointing point\n",
      "to to\n",
      "a a\n",
      "movement movement\n",
      "coalescing coalesce\n",
      "not not\n",
      "around around\n",
      "sanders sander\n",
      "but but\n",
      "around around\n",
      "his his\n",
      "resurgent resurgent\n",
      "opponent opponent\n",
      "joe joe\n",
      "biden biden\n",
      "by by\n",
      "the the\n",
      "morning morning\n",
      "after after\n",
      "the the\n",
      "hangover hangover\n",
      "had have\n",
      "worseneddata worseneddata\n",
      "from from\n",
      "several several\n",
      "states state\n",
      "suggested suggest\n",
      "that that\n",
      "sanders sander\n",
      "core core\n",
      "support support\n",
      "had have\n",
      "softened soften\n",
      "a a\n",
      "little little\n",
      "and and\n",
      "had have\n",
      "doggedly doggedly\n",
      "failed fail\n",
      "to to\n",
      "expand expand\n",
      "vermont vermont\n",
      "the the\n",
      "bastion bastion\n",
      "of of\n",
      "the the\n",
      "sanders sander\n",
      "revolution revolution\n",
      "since since\n",
      "he he\n",
      "won win\n",
      "his his\n",
      "first first\n",
      "mayoral mayoral\n",
      "election election\n",
      "in in\n",
      "burlington burlington\n",
      "exactly exactly\n",
      "years year\n",
      "ago ago\n",
      "to to\n",
      "the the\n",
      "day day\n",
      "march march\n",
      "itself itself\n",
      "told tell\n",
      "a a\n",
      "story story\n"
     ]
    }
   ],
   "source": [
    "#lemmatisation\n",
    "for numw in range(len(list_tags)):\n",
    "    tag = list_tags[numw][1]\n",
    "    if tag.startswith(\"NN\"):\n",
    "      pos = 'n'\n",
    "    elif tag.startswith('VB'):\n",
    "      pos = 'v'\n",
    "    else:\n",
    "      pos = 'a'\n",
    "    l = WordNetLemmatizer().lemmatize(list_tags[numw][0],pos)\n",
    "    print(list_tags[numw][0],l)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complément sur le tagging POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A screewriter is a person who writes screenplays\n",
      "------------------------------------------------------------\n",
      "['A', 'screewriter', 'is', 'a', 'person', 'who', 'writes', 'screenplays']\n",
      "------------------------------------------------------------\n",
      "[('A', 'DT'), ('screewriter', 'NN'), ('is', 'VBZ'), ('a', 'DT'), ('person', 'NN'), ('who', 'WP'), ('writes', 'VBZ'), ('screenplays', 'NNS')]\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def1 = 'A screewriter is a person who writes screenplays'\n",
    "tokens1 = nltk.word_tokenize(def1)\n",
    "pos_tags1 = nltk.pos_tag(tokens1)\n",
    "print(def1)\n",
    "print('---'*20)\n",
    "print(tokens1)\n",
    "print('---'*20)\n",
    "print(pos_tags1)\n",
    "print('---'*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A corporation is a large business company\n",
      "------------------------------------------------------------\n",
      "['A', 'corporation', 'is', 'a', 'large', 'business', 'company']\n",
      "------------------------------------------------------------\n",
      "[('A', 'DT'), ('corporation', 'NN'), ('is', 'VBZ'), ('a', 'DT'), ('large', 'JJ'), ('business', 'NN'), ('company', 'NN')]\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def2 = 'A corporation is a large business company'\n",
    "tokens2 = nltk.word_tokenize(def2)\n",
    "pos_tags2 = nltk.pos_tag(tokens2)\n",
    "print(def2)\n",
    "print('---'*20)\n",
    "print(tokens2)\n",
    "print('---'*20)\n",
    "print(pos_tags2)\n",
    "print('---'*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A writer is a person whose job is writing books, stories, articles, etc.\n",
      "------------------------------------------------------------\n",
      "['A', 'writer', 'is', 'a', 'person', 'whose', 'job', 'is', 'writing', 'books', ',', 'stories', ',', 'articles', ',', 'etc', '.']\n",
      "------------------------------------------------------------\n",
      "[('A', 'DT'), ('writer', 'NN'), ('is', 'VBZ'), ('a', 'DT'), ('person', 'NN'), ('whose', 'WP$'), ('job', 'NN'), ('is', 'VBZ'), ('writing', 'VBG'), ('books', 'NNS'), (',', ','), ('stories', 'NNS'), (',', ','), ('articles', 'NNS'), (',', ','), ('etc', 'FW'), ('.', '.')]\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def3 = 'A writer is a person whose job is writing books, stories, articles, etc.'\n",
    "print(def3)\n",
    "print('---'*20)\n",
    "tokens3 = nltk.word_tokenize(def3)\n",
    "print(tokens3)\n",
    "print('---'*20)\n",
    "pos_tags3 = nltk.pos_tag(tokens3)\n",
    "print(pos_tags3)\n",
    "print('---'*20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "John Winslow Irving is an American-Canadian novelist and screenwriter.\n",
      "------------------------------------------------------------\n",
      "['John', 'Winslow', 'Irving', 'is', 'an', 'American-Canadian', 'novelist', 'and', 'screenwriter', '.']\n",
      "------------------------------------------------------------\n",
      "[('John', 'NNP'), ('Winslow', 'NNP'), ('Irving', 'NNP'), ('is', 'VBZ'), ('an', 'DT'), ('American-Canadian', 'JJ'), ('novelist', 'NN'), ('and', 'CC'), ('screenwriter', 'NN'), ('.', '.')]\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def4 = 'John Winslow Irving is an American-Canadian novelist and screenwriter.'\n",
    "print(def4)\n",
    "print('---'*20)\n",
    "tokens4 = nltk.word_tokenize(def4)\n",
    "print(tokens4)\n",
    "print('---'*20)\n",
    "pos_tags4 = nltk.pos_tag(tokens4)\n",
    "print(pos_tags4)\n",
    "print('---'*20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercice 5\n",
    "Créons un classifieur bayésien pour l'analyse de sentiments.\n",
    "Pour quelques rappels, voir par exemple :\n",
    "https://www.i3s.unice.fr/~comet/SUPPORTS/Nice-EPU-GB5-IABio/slides-part4.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/cytech/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "#Corrigé de l'exercice 5: classifieur bayésien\n",
    "\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "from nltk.corpus import twitter_samples\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "import string\n",
    "\n",
    "import random\n",
    "\n",
    "import re\n",
    "\n",
    "\n",
    "from nltk import classify\n",
    "from nltk import NaiveBayesClassifier\n",
    "\n",
    "#L'objectif de ce code est de construire un classifieur \n",
    "#bayésien faisant de l'analyse de sentiments.\n",
    "#Autrement dit un classifieur qui une fois l'apprentissage\n",
    "# terminé, associe à chaque tweet qu'il reçoit en entrée \n",
    "#la classe Positive ou Negative.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def lemmatize_sentence(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in pos_tag(tokens):\n",
    "        if tag.startswith('NN'):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "        lemmatized_sentence.append(lemmatizer.lemmatize(word, pos))\n",
    "    return lemmatized_sentence\n",
    "\t\n",
    "#La fonction suivante supprime le bruit \n",
    "\n",
    "def remove_noise(tweet_tokens, stop_words = ()):\n",
    "\n",
    "    cleaned_tokens = []\n",
    "\n",
    "    for token in tweet_tokens:\n",
    "        token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n",
    "                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', token)\n",
    "        token = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", token)\n",
    "\n",
    "\n",
    "        if len(token) > 0 and token not in string.punctuation and token.lower() not in stop_words:\n",
    "            cleaned_tokens.append(token.lower())\n",
    "    return cleaned_tokens\n",
    "\t\n",
    "\t\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********** ['#FollowFriday', '@France_Inte', '@PKuchly57', '@Milipol_Paris', 'for', 'be', 'top', 'engage', 'member', 'in', 'my', 'community', 'this', 'week', ':)']\n"
     ]
    }
   ],
   "source": [
    "#Programme principal\n",
    "\n",
    "#1. D'abord découpage en tokens et lemmatisation des \n",
    "#tokens\n",
    "positive_tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\n",
    "negative_tweet_tokens = twitter_samples.tokenized('negative_tweets.json')\n",
    "\n",
    "for num in range(len(positive_tweet_tokens)):\n",
    "    positive_tweet_tokens[num] = lemmatize_sentence(positive_tweet_tokens[num])\n",
    "\n",
    "for num in range(len(negative_tweet_tokens)):\n",
    "    negative_tweet_tokens[num] = lemmatize_sentence(negative_tweet_tokens[num])\n",
    "\t\n",
    "print('***********',positive_tweet_tokens[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********** ['#followfriday', 'top', 'engage', 'member', 'community', 'week', ':)']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#2. Suppression du bruit. On considère comme du bruit \n",
    "#toute partie du texte qui n'ajoute rien à son sens.\n",
    "#Ici, on supprimera les hyperliens \n",
    "#(http://** et https://), les adresses Twitter (@xxx), \n",
    "#les signes de ponctuations, les caractères spéciaux\n",
    "#et les stopwords que nous avons introduits \n",
    "#dans le premier TP (fot, in, with, ...). Nous appelons la fonction remove_noise()\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "\n",
    "\n",
    "positive_cleaned_tokens_list = []\n",
    "negative_cleaned_tokens_list = []\n",
    "\n",
    "for tokens in positive_tweet_tokens:\n",
    "    positive_cleaned_tokens_list.append(remove_noise(tokens, stop_words))\n",
    "\n",
    "for tokens in negative_tweet_tokens:\n",
    "    negative_cleaned_tokens_list.append(remove_noise(tokens, stop_words))\n",
    "\t\n",
    "print('***********',positive_cleaned_tokens_list[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#3. On prépare la création d'un classifieur bayésien \n",
    "#dont le rôle une fois l'apprentissage terminé, associera\n",
    "# la valeur Positif ou Négatif\n",
    "#à tout tweet qui lui est soumis. \n",
    "#Pour ce faire, le code suivant représente chaque tweet \n",
    "#sous la forme (tweet, valeur) avec \n",
    "#valeur = Positive/Negative\n",
    "#et tweet est un dictionnaire où les clés sont \n",
    "#les mots du tweet et les valeurs sont toutes \n",
    "#égales à True.\n",
    "\n",
    "def get_tweets_for_model(cleaned_tokens_list):\n",
    "    for tweet_tokens in cleaned_tokens_list:\n",
    "        yield dict([token, True] for token in tweet_tokens)\n",
    "\n",
    "positive_tokens_for_model = get_tweets_for_model(positive_cleaned_tokens_list)\n",
    "negative_tokens_for_model = get_tweets_for_model(negative_cleaned_tokens_list)\n",
    "\n",
    "positive_dataset = [(tweet_dict, \"Positive\") for tweet_dict in positive_tokens_for_model]\n",
    "\n",
    "negative_dataset = [(tweet_dict, \"Negative\") for tweet_dict in negative_tokens_for_model]\n",
    "\n",
    "dataset = positive_dataset + negative_dataset\n",
    "\n",
    "#Le code suivant répartit l'ensemble des tweets en un training set (70%) et un test set (30%)\n",
    "\n",
    "random.shuffle(dataset)\n",
    "\n",
    "train_data = dataset[:7000]\n",
    "test_data = dataset[7000:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5. Création du classifieur bayésien.\n",
    "\n",
    "classifier = NaiveBayesClassifier.train(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  0.9963333333333333\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#6. Test du classifieur.\n",
    "\n",
    "acc = classify.accuracy(classifier, test_data)\n",
    "print(\"Accuracy = \", acc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Essayons un modèle NLP avec Keras !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['take', ':('])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Negative'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst_tweets_tokens = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_complete_tweets_pos = [' '.join(positive_tweet_tokens[i]) for i in range(len(positive_tweet_tokens))]\n",
    "lst_complete_tweets_neg = [' '.join(negative_tweet_tokens[i]) for i in range(len(negative_tweet_tokens))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., ..., 1., 1., 1.])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.ones(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pos = pd.DataFrame(lst_complete_tweets_pos, columns=['tweets'])\n",
    "df_neg = pd.DataFrame(lst_complete_tweets_neg, columns=['tweets'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pos['sentiment'] =  + np.ones(5000).astype('int')\n",
    "df_neg['sentiment'] =  + np.zeros(5000).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#FollowFriday @France_Inte @PKuchly57 @Milipol...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@Lamb2ja Hey James ! How odd :/ Please call ou...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@DespiteOfficial we have a listen last night :...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@97sides CONGRATS :)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>yeaaaah yippppy ! ! ! my accnt verify rqst hav...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>@chriswiggin3 Chris , that's great to hear :) ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>@RachelLiskeard Thanks for the shout-out :) It...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>@side556 Hey ! :) Long time no talk ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>@staybubbly69 as Matt would say . WELCOME TO A...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>@DanielOConnel18 you could say he will have eg...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 tweets  sentiment\n",
       "0     #FollowFriday @France_Inte @PKuchly57 @Milipol...          1\n",
       "1     @Lamb2ja Hey James ! How odd :/ Please call ou...          1\n",
       "2     @DespiteOfficial we have a listen last night :...          1\n",
       "3                                  @97sides CONGRATS :)          1\n",
       "4     yeaaaah yippppy ! ! ! my accnt verify rqst hav...          1\n",
       "...                                                 ...        ...\n",
       "4995  @chriswiggin3 Chris , that's great to hear :) ...          1\n",
       "4996  @RachelLiskeard Thanks for the shout-out :) It...          1\n",
       "4997            @side556 Hey ! :) Long time no talk ...          1\n",
       "4998  @staybubbly69 as Matt would say . WELCOME TO A...          1\n",
       "4999  @DanielOConnel18 you could say he will have eg...          1\n",
       "\n",
       "[5000 rows x 2 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hopeless for tmr :(</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Everything in the kid section of IKEA be so cu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@Hegelbon That heart slide into the waste bask...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>“ @ketchBurning : I hate Japanese call him \" b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dang start next week I have \" work \" :(</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>I wanna change my avi but uSanele :(</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>MY PUPPY BROKE HER FOOT :(</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>where's all the jaebum baby picture :( (</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>But but Mr Ahmad Maslan cook too :( https://t....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>@eawoman As a Hull supporter I be expect a mis...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 tweets  sentiment\n",
       "0                                   hopeless for tmr :(          0\n",
       "1     Everything in the kid section of IKEA be so cu...          0\n",
       "2     @Hegelbon That heart slide into the waste bask...          0\n",
       "3     “ @ketchBurning : I hate Japanese call him \" b...          0\n",
       "4               Dang start next week I have \" work \" :(          0\n",
       "...                                                 ...        ...\n",
       "4995               I wanna change my avi but uSanele :(          0\n",
       "4996                         MY PUPPY BROKE HER FOOT :(          0\n",
       "4997           where's all the jaebum baby picture :( (          0\n",
       "4998  But but Mr Ahmad Maslan cook too :( https://t....          0\n",
       "4999  @eawoman As a Hull supporter I be expect a mis...          0\n",
       "\n",
       "[5000 rows x 2 columns]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_neg, df_pos], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hopeless for tmr :(</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Everything in the kid section of IKEA be so cu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@Hegelbon That heart slide into the waste bask...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>“ @ketchBurning : I hate Japanese call him \" b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dang start next week I have \" work \" :(</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>@chriswiggin3 Chris , that's great to hear :) ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>@RachelLiskeard Thanks for the shout-out :) It...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>@side556 Hey ! :) Long time no talk ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>@staybubbly69 as Matt would say . WELCOME TO A...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>@DanielOConnel18 you could say he will have eg...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 tweets  sentiment\n",
       "0                                   hopeless for tmr :(          0\n",
       "1     Everything in the kid section of IKEA be so cu...          0\n",
       "2     @Hegelbon That heart slide into the waste bask...          0\n",
       "3     “ @ketchBurning : I hate Japanese call him \" b...          0\n",
       "4               Dang start next week I have \" work \" :(          0\n",
       "...                                                 ...        ...\n",
       "4995  @chriswiggin3 Chris , that's great to hear :) ...          1\n",
       "4996  @RachelLiskeard Thanks for the shout-out :) It...          1\n",
       "4997            @side556 Hey ! :) Long time no talk ...          1\n",
       "4998  @staybubbly69 as Matt would say . WELCOME TO A...          1\n",
       "4999  @DanielOConnel18 you could say he will have eg...          1\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(df['tweets'],\\\n",
    "                     df['sentiment'], stratify=df['sentiment'], test_size=0.1, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3049         why do she look so :( http://t.co/2NajN7LP0c\n",
       "1928                   @winnerdumb_ mind to follback ? :)\n",
       "266     @ryan2390 ever since I mention fly fish the ot...\n",
       "3557                   Why can't I be good already :( ( (\n",
       "3352            @c2c_Rail Only because it's Friday ... :D\n",
       "                              ...                        \n",
       "2715    More I look at that , the more I rather fancy ...\n",
       "4547    I need my boyfriend rn but he won't answer my ...\n",
       "2561    @_ajaaaa where r u gonna be Sarajevo ? ? ? :( ...\n",
       "2178    First time travel to expo by myself . & the ba...\n",
       "3855    @lamekyungsoo aw thank you so much ! We should...\n",
       "Name: tweets, Length: 1000, dtype: object"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppresion de la ponctuation, passage en minuscule, stemming, lemmatisation, stopwords\n",
    "\n",
    "def clean_tweet(text, flg_stemm=False, flg_lemm=True, lst_stopwords=None):\n",
    "    \"\"\"\n",
    "    Cette fonction va nettoyer le texte : \n",
    "    + mettre en minuscule, \n",
    "    + supprimer un certain nombre d'expression,\n",
    "    + choix lemmatisation / stemming,\n",
    "    + choix d'une liste de stopwords\n",
    "    \"\"\"\n",
    "    \n",
    "    text = text.lower()\n",
    "    text = text.replace('\\n', ' ').replace('\\r', '')\n",
    "    text = ' '.join(text.split())\n",
    "    text = re.sub(r\"[A-Za-z\\.]*[0-9]+[A-Za-z%°\\.]*\", \"\", text)\n",
    "    text = re.sub(r\"(\\s\\-\\s|-$)\", \"\", text)\n",
    "    text = re.sub(r\"[,\\!\\?\\%\\(\\)\\/\\\"]\", \"\", text)\n",
    "    text = re.sub(r\"\\&\\S*\\s\", \"\", text)\n",
    "    text = re.sub(r\"\\&\", \"\", text)\n",
    "    text = re.sub(r\"\\+\", \"\", text)\n",
    "    text = re.sub(r\"\\#\", \"\", text)\n",
    "    text = re.sub(r\"\\$\", \"\", text)\n",
    "    text = re.sub(r\"\\£\", \"\", text)\n",
    "    text = re.sub(r\"\\%\", \"\", text)\n",
    "    text = re.sub(r\"\\:\", \"\", text)\n",
    "    text = re.sub(r\"\\@\", \"\", text)\n",
    "    text = re.sub(r\"\\-\", \"\", text)\n",
    "    \n",
    "    ## transforme en tokens\n",
    "    lst_text = text.split()\n",
    "    ## remove Stopwords\n",
    "    if lst_stopwords is not None:\n",
    "        lst_text = [word for word in lst_text if word not in \n",
    "                    \n",
    "                    lst_stopwords]\n",
    "                \n",
    "    ## Stemming (supprime -ing, -ly, ...)\n",
    "    if flg_stemm == True:\n",
    "        ps = nltk.stem.porter.PorterStemmer()\n",
    "        lst_text = [ps.stem(word) for word in lst_text]\n",
    "                \n",
    "    ## Lemmatisation (retourne la racine du mot)\n",
    "    if flg_lemm == True:\n",
    "        lem = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "        lst_text = [lem.lemmatize(word) for word in lst_text]\n",
    "            \n",
    "    ## liste => string\n",
    "    text = \" \".join(lst_text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame(x_train)\n",
    "df_test = pd.DataFrame(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Need a shower and food first though :)'"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.iloc[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['clean'] = df_train['tweets'].apply(lambda row : clean_tweet(row, flg_stemm=False, \n",
    "                                                      flg_lemm=True, lst_stopwords=stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['clean'] = df_test['tweets'].apply(lambda row : clean_tweet(row, flg_stemm=False, \n",
    "                                                      flg_lemm=True, lst_stopwords=stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(num_words=10000,\n",
    "                     filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "                     lower=True,\n",
    "                     char_level=False #si True, chaque caractère est un token\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer.fit_on_texts(df_train['clean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_seq = tokenizer.texts_to_sequences(x_train)\n",
    "x_test_seq = tokenizer.texts_to_sequences(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "\n",
    "maxlen = 300\n",
    "\n",
    "x_train_pad = sequence.pad_sequences(x_train_seq, maxlen=maxlen)\n",
    "x_test_pad = sequence.pad_sequences(x_test_seq, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense, Bidirectional, LSTM, BatchNormalization, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(10000, 8, input_length = maxlen))\n",
    "model.add(LSTM(128, return_sequences='True'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.25))\n",
    "model.add(LSTM(64, return_sequences='True'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "282/282 [==============================] - 58s 198ms/step - loss: 0.6449 - accuracy: 0.6440\n",
      "Epoch 2/5\n",
      "282/282 [==============================] - 57s 201ms/step - loss: 0.4506 - accuracy: 0.7949\n",
      "Epoch 3/5\n",
      "282/282 [==============================] - 56s 200ms/step - loss: 0.3903 - accuracy: 0.8288\n",
      "Epoch 4/5\n",
      "282/282 [==============================] - 56s 200ms/step - loss: 0.3511 - accuracy: 0.8538\n",
      "Epoch 5/5\n",
      "282/282 [==============================] - 57s 201ms/step - loss: 0.3130 - accuracy: 0.8678\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb7222e89d0>"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train_pad, y_train, epochs=5, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       ...,\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 2s 49ms/step - loss: 0.7303 - accuracy: 0.7860\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7303022146224976, 0.7860000133514404]"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test_pad, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_glob",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7ec269740e7ea31d82f13de092c9c525d874879adfe3c5421cd7e73f5f127fdb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
